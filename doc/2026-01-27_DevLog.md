# 2026-01-27 개발 로그 (서버 아키텍처 대격변 - 구조적 안전성과 RAII)

## 1. 개요
*   **목표**: 비동기 네트워크 환경에서 **Dangling Pointer(이미 소멸된 세션 참조)** 및 **Memory Leak(패킷 누수)** 문제를 원천 봉쇄하는 아키텍처 구축.
*   **결어**: 성능과 안전성의 트레이드오프에서 C++의 **Zero-Overhead** 철학을 유지하며 **RAII(Resource Acquisition Is Initialization)** 패턴을 전격 도입하여 컴파일 타임에 안전성을 보장하는 구조로 전환.

## 2. 주요 아키텍처 변경

### 2.1. 네트워크 메모리 관리: `PacketPtr` (Intrusive Smart Pointer)
*   **문제**:
    *   `Raw Pointer` 사용 시 전송 완료 시점을 추적하기 어려워, `delete` 누락이나 중복 해제(Double Free) 위험 상존.
    *   `std::shared_ptr`는 별도의 **제어 블록(Control Block)** 할당으로 인한 오버헤드와 **이중 복사(Double Indirection)** 문제 발생.
*   **해결**:
    *   **Intrusive Reference Counting**: 패킷 객체(`Message`) 내부에 이미 존재하는 `atomic refCount`를 직접 활용하는 스마트 포인터 구현.
    *   **Zero Allocation**: 힙 할당 없이 스택 객체만으로 참조 카운팅 수행 (사실상 `Raw Pointer`와 동일한 성능).
    *   **Auto Release**: 스코프를 벗어나거나 전송 완료 시 자동 `DecRef` 및 풀 반환.

### 2.2. 서버 종료 시퀀스 및 스레드 모델 개선
*   **문제**: `Framework` 소멸 시 IO 스레드와 로직 스레드가 서로의 종료를 기다리는 **데드락(Deadlock)** 발생. `signal_set`의 비동기 핸들러가 소멸된 객체에 접근하는 이슈.
*   **해결**:
    *   **명시적 `Join()`**: 소멸자(`~Framework`)에 의존하지 않고, 명시적인 종료 함수를 두어 리소스 해제 순서(Socket -> IO Context -> Threads)를 제어.
    *   **Signal Handling 격리**: 시그널 객체를 PIMPL 패턴으로 숨기고, IO 컨텍스트와의 생명주기 의존성을 명확히 함.

### 2.3. 컴파일 최적화 및 의존성 격리 (PIMPL)
*   **변화**: `BackendSession`의 네트워크 버퍼, `asio::socket` 등 복잡한 멤버들을 `BackendSessionImpl` 구조체로 격리.
*   **효과**: 헤더 파일에서 `<boost/asio.hpp>` 의존성을 제거하여 컴파일 시간 단축 및 ABI 안정성 확보.

### 2.4. 패킷 시스템 템플릿화
*   **변화**: 반복적인 패킷 클래스 정의(`class S_LoginPacket...`)를 `ProtobufPacket<ID, Type>` 템플릿으로 통합.
*   **효과**: 수백 줄의 보일러플레이트 코드 제거 및 실수 방지.

## 3. 회고 (Retrospective)

### "더 좋은 방법은 없었는가?"
*   오늘의 작업은 **"런타임의 고통을 컴파일 타임과 구조적 제약으로 옮기는 과정"**이었다.
*   `Node.js`나 `Redis`처럼 단일 스레드로 도피하거나, 무분별한 복사(Copy)로 안전을 사는 대신, 우리는 **C++의 정수(Zero-Copy & Safety)**를 택했다.
*   이 구조는 초기 구현 난이도가 높지만, 향후 유지보수 단계에서 "실수로라도 잘못될 수 없는(Correct by Construction)" 환경을 제공한다.

## 4. 향후 과제
*   **IOCP/Epoll Native**: 현재 ASIO 기반이지만, 더 극단적인 성능이 필요하다면 OS Native API로 `Impl`만 교체 가능하도록 설계됨.
*   **Lock-Free Send Queue**: 현재의 `concurrentqueue` 외에, 세션별 `Thread-Local` 버퍼링 도입 검토 -> **(완료 확인)** `MessagePool`에 `thread_local L1Cache`가 이미 적용되어 있으며, `ConcurrentQueue` 내부적으로 스레드별 큐를 관리하므로 추가 구현 불필요.
