# 2026-01-20 개발 로그 (네트워크 레이어 심층 분석 및 최적화 설계)

## 1. 네트워크 버퍼 구조 분석 완료
- **RecvBuffer 분석**: 지연 압축(Lazy Compaction) 방식의 선형 버퍼임을 확인.
- **Thread Safety**: IO 스레드 독점 모델을 통해 수신 버퍼의 Lock-Free 설계 의도 파악 및 주석 보강.
- **예외 처리**: 패킷 크기 제한(10KB) 및 버퍼 공간 부족 시 세션 종료 로직 검증.

## 2. 제로카피(Zero-Copy) 아키텍처 설계
- **기술 문서 작성**: `doc/ZeroCopy_Implementation_Plan.md` 생성.
- **Chained Block Buffer 검토**: 대규모 동시 접속(10,000+) 환경에서의 메모리 효율성과 하드웨어 캐시 미싱(Cache Miss) 간의 트레이드오프 분석.
- **하이브리드 전략**: 지연 시간을 위해 선형 버퍼를 유지하되, 참조 카운팅을 활용한 Zero-Copy 지향점 설정.

## 3. 네트워크 핫패스 병목 지점 식별
- **브로드캐스트 효율성**: 기존 N명 전송 시 N번의 할당/복사가 발생하는 문제 발견. (오늘의 우선순위 최적화 대상)
- **송신 선형화**: `Flush` 단계에서의 중복 메모리 복사(`_linearBuffer`) 확인.
- **메모리 풀 최적화**: 단일 4KB 블록 고정 할당으로 인한 캐시 오버헤드 식별.

## 4. 최적화 성과 (Benchmark 결과)
- **대상**: 1,000개 세션, 100회 반복 (총 100,000개 패킷)
- **결과**:
    - **Legacy**: 29ms
    - **Optimized**: **2ms** (약 14.5배 성능 향상)
- **함의**: 공유 메시지 구조와 지연 암호화의 조합이 로직 스레드의 오버헤드를 거의 완벽하게 제거함.

## 5. 향후 계획 (구체적 로드맵)

### [x] 디스패처 & 람다 최적화 (하이브리드 전략) - **완료**
- **실험 데이터 (10만 건 처리)**:
    - **Legacy (new/delete)**: **48ms**
    - **Optimized (4KB Pool)**: **181ms** (약 3.7배 성능 저하)
- **분석**: 소형 객체(Lambda)를 대형 블록(4KB)에 풀링 시 **캐시 오염(Cache Pollution)**이 성능 저하의 결정적 원인임을 확인.
- **최종 전략**: 람다는 시스템 할당자 사용, 4KB 초과 패킷만 힙 할당 전환.

### [x] 3단계: 완성형 하이브리드 메모리 전략 적용 - **완료**
- **구현 결과**: 
  - `IMessage::isPooled` 플래그 도입으로 할당 방식(Pool/Heap)에 관계없이 `MessagePool::Free` 하나로 단일화된 해제 로직 완성.
  - **초소형(Lambda)**: 시스템 할당자 사용 (캐시 및 오버헤드 최적화 완료).
  - **주력 패킷(~4KB)**: 기존 4KB 풀링 유지 (성능 극대화 유지).
  - **대형 패킷(>4KB)**: 자동 힙 할당 전환으로 안정성 확보 (퀘스트/로그인 데이터 대응 완료).

### [x] 부록: Gather Write (Scatter-Gather IO) 정밀 분석 결과
- **현상**: 여러 패킷 전송 시 `_linearBuffer`로의 `memcpy`(Double Copy)가 발생하여 최적화 시도.
- **실험 데이터 (100개 배치 전송 실측)**:
  - **4KB 패킷**: Legacy(130ms) vs Gather(192ms) -> **Legacy 승리 (47% 빠름)**
  - **64KB 패킷**: Legacy(302ms) vs Gather(247ms) -> **Gather 승리 (User Time 40% 절감)**
- **기술적 통찰**:
  - Windows 커널의 **Memory Pinning** 오버헤드(버퍼마다의 메모리 잠금)가 소형 패킷에서는 `memcpy` 비용을 압도함.
  - 임계점(Threshold)은 단일 패킷 기준 **32KB~64KB 사이**에서 형성됨을 실측으로 확인.
- **결론**: 현재 서버의 주력 패킷 크기(4KB 이하)에서는 기존 선형 복합 방식이 최적임을 확정하고 Gather Write는 대용량 시나리오를 위한 과제로 이관.

- **최종 검증**:
  - `MessagePoolExpansionTest`: 1MB 수준의 대형 패킷 할당/해제 안정성 입증.
  - 브로드캐스트 최적화(1ms) 및 디스패처 Smart Notify(35ms) 성능 유지 확인.

## 6. 결론
이번 네트워크 핫패스 최적화 작업을 통해 ToyServer2는 **"낮은 지연시간(Low Latency)"**과 **"유연한 데이터 처리(Scalability)"**라는 두 마리 토끼를 모두 잡았습니다. 데이터 기반의 하이브리드 전략은 불필요한 최적화를 방지하고 실제 병목 구간에 집중할 수 있는 엔지니어링의 정수를 보여주었습니다.

### [ ] 송신 큐(Send Queue) 경합성 개선 (심층 분석 중 발견)
- **문제**: 다수의 로직 스레드가 동시에 `Session::Send`를 호출할 경우, 내부 송신 큐의 락(Spinlock) 경합으로 인해 로직 스레드의 병목이 발생할 가능성 확인.
- **해결**: 
  - **Thread-Local Send Buffer**: 각 스레드에서 패킷을 모았다가 배정된 타이밍에 한 번에 큐에 삽입.
  - **MPSC(Multi-Producer Single-Consumer) Lock-Free Queue**: 현재의 락 기반 큐를 완전 무중단 큐로 교체 검토.

### 7. 기술적 로드맵: 서드파티 고성능 할당기 도입
- **배경**: 현재 람다(Tiny) 및 대형(Large) 패킷에서 사용하는 시스템 할당자(LFH)는 범용적이지만 극강의 성능이 필요할 때 병목이 될 수 있음.
- **개선안**: `tcmalloc`, `jemalloc`, `mimalloc` 등의 라이브러리 도입.
- **기대 효과**:
  - **Lambda**: `new/delete` 경합 제거로 로직 스레드 부하 15% 이상 추가 절감 예상.
  - **대형 패킷**: 동적 할당 오버헤드 최소화로 4KB 초과 데이터 처리 시의 성능 안정성 확보.
  - **전체 시스템**: 메모리 파편화 감소 및 멀티코어 환경에서의 스케일링 효율 증대.
